Convolutional neural networks were first proposed for image
recognition by LeCun et al.\ \cite{lecun1989backpropagation} but have
gained wider recognition after the ImageNet 2012
competition \cite{krizhevsky2012imagenet}. In this work we use 3D
convolutional networks to score protein structures. The architecture
of the model is shown in Fig.~\ref{Fig:CNNModel}.  It is comprised of
three blocks of alternating convolutional, volumetric batch
normalization, and ReLU layers, followed by three fully-connected
layers with ReLU nonlinearities. The final output of the network is a
single number, interpreted as the score of the input structure.

\begin{figure}[H]
    \centering
    \includegraphics[width=\linewidth]{Fig/ConvnetDiagramV1.eps}

    \caption{Schematic representation of the convolutional neural
    network architecture used in this work. Line connections across
    boxes denote the consecutive application of a 3D convolutional
    layer (``Convolution''), a batch normalization layer
    (``BatchNorm''), and a ReLU layer. Arrows between boxes denote
    maximum pooling layers (``MaxPool''). The labels ``$\times M$''
    denote the number of filters used in the corresponding 3D
    convolutional layer. The size of all filters and maximum pooling
    domains are $3\times 3\times 3$. The grey stripes denote
    one-dimensional vectors and crossed lines between them stand for
    fully-connected layers with ReLU non-linearities. Details of the
    model parameters can be found in Supporting Information Table~S4.}

    \label{Fig:CNNModel}
\end{figure}

Each 3D convolutional layer takes $N$ input density maps $f$ and
transforms them using $M$ filters $F$ according to the following
formula:
$$
f^\text{out}_i (\mathbf{r}) = \sum^{N}_{j=1} \int F_i (\mathbf{r} - \mathbf{r'}) \cdot f^\text{in}_j(\mathbf{r'}) ~d\mathbf{r'}, \quad\forall i \in [1,M]
$$
In practice, these convolutions are approximated by sums on a 3D grid.
The ReLU nonlinearity is computed as following:
$$
f^\text{out}_i (\mathbf{r}) = \begin{cases}
               f^\text{in}_i(\mathbf{r}) &\text{if } f^\text{in}_i(\mathbf{r})\geq 0\\
               0                         &\text{if } f^\text{in}_i(\mathbf{r})<0\\
            \end{cases}, \quad\forall i \in [1,M]
$$
The concept of batch normalization layer was introduced by Ioffe and
Szegedy \cite{ioffe2015batch} to address the problem of internal
covariate shift (the shift in the distribution of subnetwork outputs
during training). In practice, this layer normalizes each input value
according to the mean and variance of the corresponding input within
the subset of examples used to estimate the gradient (``batch''):
$$
\hat{f}^\text{in}_k(\mathbf{r}) = \frac{f^\text{in}(\mathbf{r}) - \mu_\text{B}}{\sqrt{\sigma^{2}_\text{B} + \epsilon}}, \quad\forall k \in [1,N_\text{B}]
$$
where $\mu_\text{B}(\mathbf{r})
= \frac{1}{N_\text{B}} \sum_{k=1}^{N_\text{B}}
f^\text{in}_i(\mathbf{r})$, and $\sigma^{2}_\text{B}
= \frac{1}{N_\text{B}} \sum_{k=1}^{N_\text{B}} \left(
f^\text{in}_i(\mathbf{r}) - \mu_\text{B}
(\mathbf{r}) \right)^2$. $N_\text{B}$ is the number of examples in the
minibatch. The constant $\epsilon = ???$ is added to avoid division by
zero. Afterwards, the output of this layer is computed by scaling the
normalized inputs:
$$
f^\text{out}_k(\mathbf{r}) = \gamma \hat{f}^\text{in}_k(\mathbf{r}) + \beta, \quad\forall k \in [1,N_B]
$$
The parameters $\gamma$ and $\beta$ are learned along with other
parameters of the network during the training.

The maximum pooling layer (``MaxPool'') is used to build a
coarse-grained representation of the input. The output of this layer
is the maximum over the cubes of size $d \times d \times d$ that cover
the input domain with the stride $l$ (distance between two consequtive
cubes along one dimension).  The output size is then approximately $l$
times smaller than the input data bounding box.  All three ``MaxPool''
layers of the model (Fig.~\ref{Fig:CNNModel}) use $d=3$ and $l=2$.

During the coarse-graining procedure, the size of the individual data
grids eventually shrinks to a single cell. The flattening layer
reshapes the array of $1\times 1\times 1$ density maps to a single
vector. Afterwards, we compute several transformations using
fully-connected layers. These layers transform a vector
$\mathbf{x}_\text{in}$ as following:
$$
\mathbf{x}_\text{out} = W \cdot \mathbf{x}_\text{in} + \mathbf{b}
$$
where $W$ is a matrix and $\mathbf{b}$ is a vector, learned during the
training. Each output vector is then transformed by a ReLU layer.
